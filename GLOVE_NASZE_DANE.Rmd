Przygotowanie danych
```{r}
library(stringr)

setwd("D:/PW-MAGISTERSKIE/PW-Magisterskie-sem2/ZUM/Projekt/ZUM_Proj")
df <- read.csv(
  file = 'Data/winemag-data-130k-v2.csv',
  sep = ','
)
# Odrzucenie przykładów nie posiadających wartości w kolumnie opisu
df_processed <- df[,colSums(is.na(df["description"]))<nrow(df)]

# Zamiana wszyskich znaków na małe znaki
df_processed[["description"]] <- tolower(df_processed[["description"]])

# Odrzucenie wszyskich znaków specjalnych oraz liczb poza znakiem "'" specyficznym dla języka ang
df_processed[["description"]] <- str_replace(gsub("([^a-z'])", 
                                                  " ", 
                                                  df_processed[["description"]]), 
                                             "B", 
                                             "b") 

# Usunięcie powtórzonych spacji
df_processed[["description"]] <-str_replace(gsub("\\s+",
                                                 " ",
                                                 df_processed[["description"]]),
                                              "B",
                                              "b")

# Usunięcie spacji na końcu opisu
df_processed[["description"]] <-str_replace(gsub("\\s$", "", df_processed[["description"]]), "B", "b")
```

Robienie słownika i tokenóW 
```{r}
library(text2vec)
tokens = word_tokenizer(df_processed$description)
it = itoken(tokens, ids = df_processed$X, progressbar = FALSE)
vocab = create_vocabulary(it)
```

Tworzenie TCM
tcm: Term-co-occurence matrix construction
```{r}
vectorizer = vocab_vectorizer(vocab)
tcm = create_tcm(it, vectorizer, skip_grams_window = 5L)
```
 
Fit and transform GLOVE
```{r}
glove = GlobalVectors$new(rank = 100, x_max = 10)
wv_main = glove$fit_transform(tcm, n_iter = 100, convergence_tol = 0.01, n_threads = 12);
```
 
Tworzenie wektorów
```{r}
wv_context = glove$components
word_vectors = wv_main + t(wv_context)
```
 
Sprawdzenie poprawności działania
```{r}
fruit = word_vectors["fruit", , drop = FALSE] + word_vectors["black", , drop = FALSE]
cos_sim = sim2(x = word_vectors, y = fruit, method = "cosine", norm = "l2") 
head(sort(cos_sim[,1], decreasing = TRUE), 5)
```
Tworzenie wektorów dla dokumentów 
Dla wszystkich tokenóW pobierane są występujące w nich wektory słów a następnie liczona jest średnia z wartości kolumn (kolumna-współrzędna wektora). Tak powstały wektor wpisywany jest do listy
```{r}
t1 = Sys.time()
doc_vectors_list <- lapply(tokens, function(x) colMeans(word_vectors[x,]))
print(difftime(Sys.time(), t1, units = 'sec'))
```
Przetworzenie listy wektorów do dataframe
```{r}
doc_vectors_df <- data.frame(matrix(unlist(doc_vectors_list), nrow=length(doc_vectors_list), byrow=TRUE))
```

Zapis wyników
```{r}
write.csv(doc_vectors_df,"GLOVE_NASZE_DANE.txt", row.names = TRUE)
```

Jak dokładnie działa tworzenie wektorów dokumentu?
<!-- ```{r} -->
<!-- writeLines('lista słów dokumentu') -->
<!-- writeLines(train_tokens[[1]]) # zawiera listę słów w danym dokumentcie -->
<!-- writeLines("\n") -->
<!-- writeLines("wektory dla poszczególnych słów") -->
<!-- word_vectors[train_tokens[[1]],] #lista zawierająca vektory dla poszczególnych słów -->
<!-- writeLines("\n") -->
<!-- writeLines("Srednia wartość z współrzędnych wektorów -> wektor dokumentu") -->
<!-- print(colMeans(word_vectors[train_tokens[[1]],])) -->
<!-- ``` -->